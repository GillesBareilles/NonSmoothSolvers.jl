#+TITLE: NonSmoothSolvers.jl

This package implements several algorithms for nonsmooth (nonconvex) optimization, including Gradient Sampling, nonsmooth BFGS and nonconvex proximal point. The implementations follow closely the related papers, tuning parameters is advisable to get the best of the methods. Comments and suggestions are more than welcome, get in touch with GitHub issues or by mail!

This is a work in progress. In particular, there may be bugs in algorithms, and the todo list is:
- [ ] list features of ~optimize!~;
- [ ] ~GradientSampling~: solve subproblem with ~ConvexHullProjection~ (with warmstart?)
- [ ] describe interactions with other packages (NonSmoothProblems, OptimPlots, ...);
- [ ] bring in nonconvex prox and proximal point;
- [ ] add acceleration methods of proximal point;
- [ ] test optimization methods;

*Setup* is as follows: in a julia REPL (default environment):
#+begin_src julia
]add https://github.com/GillesBareilles/ConjugateGradient.jl https://github.com/GillesBareilles/ConvexHullProjection.jl https://github.com/GillesBareilles/NonSmoothProblems.jl https://github.com/GillesBareilles/NonSmoothSolvers.jl
#+end_src

* Algorithms for general nonsmooth optimization

A general nonsmooth optimization problem may be implemented following ~NonSmoothProblems.jl~' interface:
#+begin_src julia
using NonSmoothProblems
using NonSmoothSolvers

struct Simplel1 <: NonSmoothProblems.NonSmoothPb end

F(::Simplel1, x) = norm(x, 1)
∂F_elt(::Simplel1, x) = sign.(x)
∂F_minnormelt(::Simplel1, x) = sign.(x)
is_differentiable(::Simplel1, x) = length(filter(xᵢ -> xᵢ == 0, x)) > 0
#+end_src

** Solvers
The generic solvers implemented are:
- Gradient Sampling, following
  #+begin_quote
*Gradient Sampling Methods for Nonsmooth Optimization*, J.V. Burke, F.E. Curtis, A.S. Lewis, M.L. Overton, L.E.A. Simões, 2018.
  #+end_quote
- Nonsmooth BFGS, following
  #+begin_quote
*Nonsmooth optimization via quasi-Newton methods*, Adrian S. Lewis, Michael L. Overton, 2012
  #+end_quote

** Examples
They are used as follows:
#+begin_src julia
n = 10
pb = Simplel1()
x = rand(n)
optparams = OptimizerParams(iterations_limit=20, trace_length=20)

o = GradientSampling(x)
xfinal_gs, tr = optimize!(pb, o, x; optparams)

o = NSBFGS()
xfinal_nsbfgs, tr = optimize!(pb, o, x; optparams)
#+end_src

Other problems:
- the historical /MaxQuad/, /Numerical  Optimisation/, BGLS, p. 153 (2nd edition):
#+begin_src julia
pb = MaxQuadBGLS()
x = zeros(10) .+ 1
#+end_src
- a maximum of two quadratics:
#+begin_src julia
pb = MaxQuadAL()
x = zeros(2) .+ 1
#+end_src

** Input parameters, output values

The ~optimize!~ function accepts:
- several convergence checkers, possibly defined by the user;
- several policies of data recording and displaying along iterations;

The output is the (final) point generated by the optimization procedure, and a trace object. This object is a vector of ~OptimizationState~, an immutable object which holds relevant indicators values at the end of an iteration, such as ~it~, ~time~, ~F_x~, ~norm_step~. The user can add his custom indicators by passing a callback function to ~optimize!~ (TODO).

** Notes
Method calls may be (roughly) timed using ~TimerOutputs~. This functionality is turned on/off by calling ~TimerOutputs.enable_debug_timings(NonSmoothProblems)~ and ~TimerOutputs.disable_debug_timings(NonSmoothProblems)~.
