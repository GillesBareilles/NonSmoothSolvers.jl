#+TITLE: NonSmoothSolvers.jl

[[https://GillesBareilles.github.io/NonSmoothSolvers.jl/stable][https://img.shields.io/badge/docs-stable-blue.svg]]
[[https://GillesBareilles.github.io/NonSmoothSolvers.jl/dev][https://img.shields.io/badge/docs-dev-blue.svg]]
[[https://github.com/GillesBareilles/NonSmoothSolvers.jl/actions/workflows/CI.yml?query=branch%3Amaster][https://github.com/GillesBareilles/NonSmoothSolvers.jl/actions/workflows/CI.yml/badge.svg?branch=master]]
[[https://codecov.io/gh/GillesBareilles/NonSmoothSolvers.jl][https://codecov.io/gh/GillesBareilles/NonSmoothSolvers.jl/branch/master/graph/badge.svg]]
[[https://github.com/invenia/BlueStyle][https://img.shields.io/badge/code%20style-blue-4495d1.svg]]

This package implements several algorithms for nonsmooth (nonconvex) optimization, including Gradient Sampling, nonsmooth BFGS and nonconvex proximal point.
The implementations follow closely the related papers, tuning parameters is advisable to get the best of the methods.
Comments and suggestions are more than welcome, get in touch by opening an issue!

*Setup* is as follows: in a julia REPL (default environment):
#+begin_src julia
]add https://github.com/GillesBareilles/NonSmoothProblems.jl https://github.com/GillesBareilles/NonSmoothSolvers.jl
#+end_src

* Solvers
The generic solvers implemented are:
- Gradient Sampling
  #+begin_quote
*Gradient Sampling Methods for Nonsmooth Optimization*, J.V. Burke, F.E. Curtis, A.S. Lewis, M.L. Overton, L.E.A. Sims, 2018.
  #+end_quote
- Nonsmooth BFGS
  #+begin_quote
*Nonsmooth optimization via quasi-Newton methods*, Adrian S. Lewis, Michael L. Overton, 2012
  #+end_quote
- Projection of a point on a polytope
  #+begin_quote
  *Finding the Nearest Point in A Polytope*, Philip Wolfe, Mathematical Programming, 1976
  #+end_quote

* Usage
** Defining a problem
A general nonsmooth optimization problem may be implemented following [[https://github.com/GillesBareilles/NonSmoothProblems.jl][~NonSmoothProblems.jl~]]' interface:
#+begin_src julia
using NonSmoothProblems
using NonSmoothSolvers

struct Simplel1 <: NonSmoothProblems.NonSmoothPb end

F(::Simplel1, x) = norm(x, 1)
∂F_elt(::Simplel1, x) = sign.(x)
is_differentiable(::Simplel1, x) = length(filter(xᵢ -> xᵢ == 0, x)) > 0
#+end_src

Note that [[https://github.com/GillesBareilles/NonSmoothProblems.jl][~NonSmoothProblems.jl~]] implements some nonsmooth problems (maximum of quadratics, maximum eigenvalue).

** Running a solver

#+begin_src julia
n = 10
pb = Simplel1()
x = rand(n)
optparams = OptimizerParams(iterations_limit=20, trace_length=20)

o = GradientSampling(x)
xfinal_gs, tr = optimize!(pb, o, x; optparams)

o = NSBFGS()
xfinal_nsbfgs, tr = optimize!(pb, o, x; optparams)
#+end_src

Other problems:
- the historical /MaxQuad/, /Numerical  Optimisation/, BGLS, p. 153 (2nd edition):
#+begin_src julia
pb = MaxQuadBGLS()
x = zeros(10) .+ 1
#+end_src
- a maximum of two quadratics:
#+begin_src julia
pb = MaxQuadAL()
x = zeros(2) .+ 1
#+end_src

** Input parameters, output values

The ~optimize!~ function accepts:
- several convergence checkers, possibly defined by the user;
- several policies of data recording and displaying along iterations;

The output is the (final) point generated by the optimization procedure, and a trace object. This object is a vector of ~OptimizationState~, an immutable object which holds relevant indicators values at the end of an iteration, such as ~it~, ~time~, ~F_x~, ~norm_step~. The user can add his custom indicators by passing a callback function to ~optimize!~:
#+begin_src julia
getx(optimizer, optimizerstate) = deepcopy(os.x)
optimstate_extensions = OrderedDict{Symbol, Function}(:x => getx)

o = NSBFGS()
xfinal_nsbfgs, tr = optimize!(pb, o, x; optparams, optimstate_extensions)
#+end_src

** Notes
Method calls may be (roughly) timed using ~TimerOutputs~. This functionality is turned on/off by calling ~TimerOutputs.enable_debug_timings(NonSmoothProblems)~ and ~TimerOutputs.disable_debug_timings(NonSmoothProblems)~.
*** Efficiency of nsBFGS
Profiling of the nsBFGS shows that more than 90% of the time is spent in the line search, calling oracles `F`, `∂F_elt` and `is_differentiable`.

In some cases it faster to compute all these at once rather than separately.
```julia
using NonSmoothProblems, NonSmoothSolvers, StatProfilerHTML
pb = MaxQuadBGLS(Float64)
o = NSBFGS{Float64}()
function toto(n)
    for i = 1:n
        state=NSS.initial_state(o, ones(10), pb)
        NSS.update_iterate!(state, o, pb)
    end
end
toto(1); @profilehtml toto(10000)
```

*** Efficiency of gradient sampling
Same method as above, with `o = GradientSampling(x)`.

Most of the time is spent with the resolution of the QP.


* Notes
This is a work in progress. In particular, there may be bugs in algorithms, and the todo list is:
- [ ] list features of ~optimize!~;
# - [X] ~GradientSampling~: solve subproblem with ~ConvexHullProjection~ (with warmstart?)
- [ ] describe interactions with other packages (NonSmoothProblems, OptimPlots, ...);
# - [ ] bring in nonconvex prox and proximal point;
# - [ ] add acceleration methods of proximal point;
- [ ] test optimization methods;
