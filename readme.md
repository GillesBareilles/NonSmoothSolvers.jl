# NonSmoothSolvers.jl

This package implements several algorithms for nonsmooth (nonconvex) optimization, including Gradient Sampling, nonsmooth BFGS and nonconvex proximal point. The implementations follow closely the related papers, tuning parameters is advisable to get the best of the methods. Comments and suggestions are more than welcome, get in touch with GitHub issues or by mail!

This is a work in progress. In particular, there may be bugs in algorithms, and the todo list is:
- list features of `optimize!`;
- `GradientSampling`: solve subproblem with `ConvexHullProjection` (with warmstart?)
- describe interactions with other packages (NonSmoothProblems, OptimPlots, ...);
- bring in nonconvex prox and proximal point;
- add acceleration methods of proximal point;
- test optimization methods;

**Setup** is as follows: after cloning `NonSmoothProblems.jl` and `NonSmoothSolvers.jl`, in a julia REPL (default environment):
```julia
]dev --local \path\to\NonSmoothProblems.jl
]dev --local \path\to\NonSmoothSolvers.jl
```

## Algorithms for general nonsmooth optimization

A general nonsmooth optimization problem may be implemented following `NonSmoothProblems.jl`' interface:
```julia
struct Simplel1 <: NonSmoothProblems.NonSmoothPb end

F(::Simplel1, x) = norm(x, 1)
∂F_elt(::Simplel1, x) = sign.(x)
∂F_minnormelt(::Simplel1, x) = sign.(x)
is_differentiable(::Simplel1, x) = length(filter(xᵢ -> xᵢ == 0, x)) > 0
```

The generic solvers implemented are:
- Gradient Sampling, following
> *Gradient Sampling Methods for Nonsmooth Optimization*, J.V. Burke, F.E. Curtis, A.S. Lewis, M.L. Overton, L.E.A. Simões, 2018.
- Nonsmooth BFGS, following
> *Nonsmooth optimization via quasi-Newton methods*, Adrian S. Lewis, Michael L. Overton, 2012

They are used as follows:
```julia
n = 10
pb = Simplel1()
x = rand(n)
optparams = OptimizerParams(iterations_limit=20, trace_length=20)

o = GradientSampling(x)
xfinal_gs, tr = optimize!(pb, o, x, optparams=optparams)

o = NSBFGS()
xfinal_nsbfgs, tr = optimize!(pb, o, x, optparams=optparams)
```

Other problems:
- the historical 'MaxQuad', *Numerical  Optimisation*, BGLS, p. 153 (2nd edition):
```julia
pb = MaxQuadBGLS()
x = zeros(10) .+ 1
```
- a maximum of two quadratics:
```julia
pb = MaxQuadAL()
x = zeros(2) .+ 1
```

## Input parameters, output values

The `optimize!` function accepts:
- several convergence checkers, possibly defined by the user;
- several policies of data recording and displaying along iterations;

The output is the (final) point generated by the optimization procedure, and a trace object. This object is a vector of `OptimizationState`, an immutable object which holds relevant indicators values at the end of an iteration, such as `it`, `time`, `F_x`, `norm_step`. The user can add his custom indicators by passing a callback function to `optimize!` (TODO).
## Notes
Method calls may be (roughly) timed using `TimerOutputs`. This functionality is turned on/off by calling `TimerOutputs.enable_debug_timings(NonSmoothProblems)` and `TimerOutputs.disable_debug_timings(NonSmoothProblems)`.
