
Base.@kwdef struct GradientSampling{Tf} <: Optimizer{Tf}
    m::Int64
    Œ≤::Tf = 1e-4
    Œ≥::Tf = 0.5
    œµ_opt::Tf = 1e-6
    ŒΩ_opt::Tf = 1e-6
    Œ∏_œµ::Tf = 0.1
    Œ∏_ŒΩ::Tf = 0.1
    ls_maxit::Int64 = 70
end

GradientSampling(initial_x::AbstractVector) = GradientSampling(m=length(initial_x) * 2)

Base.@kwdef mutable struct GradientSamplingState{Tf} <: OptimizerState{Tf}
    x::Vector{Tf}
    ‚àÇg·µ¢s::Matrix{Tf}
    œµ‚Çñ::Tf
    ŒΩ‚Çñ::Tf
    k::Int64 = 1
end

function initial_state(gs::GradientSampling{Tf}, initial_x::Vector{Tf}, pb) where {Tf}
    return GradientSamplingState(
        x = initial_x,
        ‚àÇg·µ¢s = zeros(Tf, length(initial_x), gs.m+1),
        œµ‚Çñ = Tf(0.1),
        ŒΩ‚Çñ = Tf(0.1),
    )
end


#
### Printing
#
print_header(gs::GradientSampling) = println("**** GradientSampling algorithm\nm = $(gs.m)")

display_logs_header_post(gs::GradientSampling) = print("||g·µè||     œµ‚Çñ       ŒΩ‚Çñ        it_ls")
function display_logs_post(os, gs::GradientSampling)
    @printf "%.3e  %.1e  %.1e  %2i" os.additionalinfo.g·µè_norm os.additionalinfo.œµ‚Çñ os.additionalinfo.ŒΩ‚Çñ os.additionalinfo.it_ls
end


#
### GradientSampling method
#
"""
    update_iterate!(state, gs::GradientSampling, pb)

NOTE: each iteration is costly. This can be explored with NonSmoothProblems.to.
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Time                   Allocations
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Tot / % measured:                  45.5s / 2.89%           1.63GiB / 14.5%

Section                            ncalls     time   %tot     avg     alloc   %tot      avg
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
GS 2. minimum norm (sub)gradient       20    1.30s  98.6%  64.9ms    240MiB  99.0%  12.0MiB
GS 4. Update parameters                20   12.8ms  0.97%   641Œºs   1.63MiB  0.67%  83.6KiB
GS 5. diff check                       20   3.04ms  0.23%   152Œºs    428KiB  0.17%  21.4KiB
GS 1. point sampling                   20   2.49ms  0.19%   124Œºs    497KiB  0.20%  24.9KiB
GS 3. Termination                      20   33.8Œºs  0.00%  1.69Œºs      320B  0.00%    16.0B
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"""
function update_iterate!(state::GradientSamplingState{Tf}, gs::GradientSampling, pb) where Tf
    iteration_status = iteration_completed
    ‚àÇg·µ¢s = state.‚àÇg·µ¢s

    @timeit_debug "GS 1. sampling points, eval gradients" begin
    ## 1. Sample m points in ùîπ(x, œµ‚Çñ)
    samplegradients!(‚àÇg·µ¢s, pb, state.x, state.œµ‚Çñ)
    end

    ## 2. Find minimal norm element of convex hull at gradients of previous points.
    @timeit_debug "GS 2. minimum norm (sub)gradient" begin
    alphaOSQP = find_minimumnormelt_OSQP(‚àÇg·µ¢s)
    # printstyled("--> CHP\n", color = :red)
    # @time alphaCHP = find_minimumnormelt_CHP(‚àÇg·µ¢s)
    # @show alphaOSQP
    # @show alphaCHP

    g·µè = ‚àÇg·µ¢s * alphaOSQP
    g·µè_norm = norm(g·µè)
    end


    ## 3. termination
    @timeit_debug "GS 3. Termination" begin
    if g·µè_norm ‚â§ gs.ŒΩ_opt && state.œµ‚Çñ ‚â§ gs.œµ_opt
        iteration_status = problem_solved
        @info "termination condition found"
    end
    end

    ## 4. Update parameters
    @timeit_debug "GS 4. Update parameters" begin
    ŒΩ_next = state.ŒΩ‚Çñ
    œµ_next = state.œµ‚Çñ
    t‚Çñ = 1.0
    it_ls = 0
    if g·µè_norm ‚â§ state.ŒΩ‚Çñ
        ŒΩ_next = gs.Œ∏_ŒΩ * state.ŒΩ‚Çñ
        œµ_next = gs.Œ∏_œµ * state.œµ‚Çñ
        t‚Çñ = 0.0
        @info "reducing sampling size"
    else
        # This test is not costly, and may help detect difficult cases
        gtd = dot(g·µè, ‚àÇF_elt(pb, state.x))
        if gtd <= 0
            @warn "not descent direction, gtd = $gtd"
        end

        ŒΩ_next = state.ŒΩ‚Çñ
        œµ_next = state.œµ‚Çñ
        t‚Çñ = 1.0

        f‚Çñ = F(pb, state.x)
        while !(F(pb, state.x - t‚Çñ * g·µè) < f‚Çñ - gs.Œ≤ * t‚Çñ * g·µè_norm^2)
            t‚Çñ *= gs.Œ≥
            it_ls += 1

            (it_ls > gs.ls_maxit) && break
            (norm(g·µè) * t‚Çñ < 10*eps(Tf)) && break
        end

        (it_ls == gs.ls_maxit) && @warn("GradientSampling(): linesearch exceeded $(gs.ls_maxit) iterations, no suitable steplength found.")
        (norm(g·µè) * t‚Çñ < 10*eps(Tf)) && @warn("Linesearch reached numerical precision")
    end
    end

    @timeit_debug "GS 5. diff check" begin
    x_next = state.x - t‚Çñ * g·µè
    if !is_differentiable(pb, x_next)
        @error("Gradient sampling: F not differentiable at next point, portion to be implemented.")
    end

    state.œµ‚Çñ = œµ_next
    state.ŒΩ‚Çñ = ŒΩ_next
    state.x = x_next
    state.k += 1
    end

    return (;
            œµ‚Çñ = state.œµ‚Çñ,
            ŒΩ‚Çñ = state.ŒΩ‚Çñ,
            it_ls,
            g·µè_norm,
            F = 2 + it_ls,              # orcale calls
            ‚àÇF_elt = gs.m+1,
            is_differentiable = 1,
            ), iteration_status
end

get_minimizer_candidate(state::GradientSamplingState) = state.x


using SparseArrays
function find_minimumnormelt_OSQP(‚àÇg·µ¢s)
    n, nsamples = size(‚àÇg·µ¢s)

    P = sparse(‚àÇg·µ¢s' * ‚àÇg·µ¢s)
    q = zeros(nsamples)
    A = sparse(vcat(Diagonal(1.0I, nsamples), ones(nsamples)'))
    l = zeros(nsamples+1)
    l[end] = 1
    u = Inf * ones(nsamples+1)
    u[end] = 1

    # Solve problem
    options = Dict(:verbose => false,
                   :polish => true,
                   :eps_abs => 1e-06,
                   :eps_rel => 1e-06,
                   :max_iter => 5000)
    model = OSQP.Model()
    OSQP.setup!(model; P=P, q=q, A=A, l=l, u=u, options...)
    results = OSQP.solve!(model)
    return results.x
end

function find_minimumnormelt_CHP(‚àÇg·µ¢s::Matrix{Tf}) where Tf
    set = CHP.SimplexShadow(‚àÇg·µ¢s)
    x0 = zeros(Tf, size(‚àÇg·µ¢s, 2))

    showtermination = true
    showtrace = true
    showls = false
    Œ±, str = CHP.optimize(set, x0; showtermination, showtrace, showls, maxiter = 13)
    return Œ±
end

function samplegradients!(‚àÇg·µ¢s, pb, x, œµ‚Çñ)
    n = size(‚àÇg·µ¢s, 1)
    nsamples = size(‚àÇg·µ¢s, 2) - 1

    for i in 1:nsamples
        ‚àÇg·µ¢ = @view ‚àÇg·µ¢s[:, i]
        ‚àÇg·µ¢ .= rand(MvNormal(zeros(n), ScalMat(n, 1.0)))
        ‚àÇg·µ¢ .*= œµ‚Çñ * rand()^(1/n) / norm(‚àÇg·µ¢)
        ‚àÇg·µ¢ .+= x
        ‚àÇg·µ¢ .= ‚àÇF_elt(pb, ‚àÇg·µ¢)
    end
    ‚àÇg·µ¢s[:, end] .= ‚àÇF_elt(pb, x)
    return
end
