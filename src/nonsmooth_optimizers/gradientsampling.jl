
Base.@kwdef struct GradientSampling <: NonSmoothOptimizer
    m::Int64
    Î²::Float64 = 1e-4
    Î³::Float64 = 0.5
    Ïµ_opt::Float64 = 1e-2
    Î½_opt::Float64 = 1e-6
    Î¸_Ïµ::Float64 = 0.1
    Î¸_Î½::Float64 = 0.1
    ls_maxit::Int64 = 50
end

GradientSampling(initial_x::AbstractVector) = GradientSampling(m=length(initial_x)+1)

Base.@kwdef mutable struct GradientSamplingState{Tx}
    x::Tx
    xs::Vector{Tx}
    Ïµâ‚–::Float64 = 0.1
    Î½â‚–::Float64 = 0.1
    k::Int64 = 1
end

function initial_state(gs::GradientSampling, initial_x, pb)
    return GradientSamplingState(
        x = initial_x,
        xs = Vector([initial_x for i in 1:gs.m]),
    )
end


#
### Printing
#
print_header(gs::GradientSampling) = println("**** GradientSampling algorithm\nm = $(gs.m)")

display_logs_header_post(gs::GradientSampling) = print("||gáµ||     Ïµâ‚–       Î½â‚–        it_ls")
function display_logs_post(os, gs::GradientSampling)
    @printf "%.3e  %.1e  %.1e  %2i" os.additionalinfo.gáµ_norm os.additionalinfo.Ïµâ‚– os.additionalinfo.Î½â‚– os.additionalinfo.it_ls
end


#
### GradientSampling method
#
function update_iterate!(state, gs::GradientSampling, pb)
    iteration_status = iteration_completed

    ## 1. Sample m points in ð”¹(x, Ïµâ‚–)
    Random.seed!(123 + state.k)
    for i in 1:gs.m
        state.xs[i] .= rand(Normal(), size(state.x))
        state.xs[i] .*= rand()^(1/length(state.x)) / norm(state.xs[i])
        state.xs[i] .*= state.Ïµâ‚–
    end

    ## 2. Find minimal norm element of convex hull at gradients of previous points.
    model = Model(with_optimizer(OSQP.Optimizer; polish=true, verbose=false, max_iter=1e8, eps_abs=1e-8, eps_rel=1e-8))

    # TODO: check complexity of this part
    t = @variable(model, 0 <= t[1:gs.m+1] <= 1)
    gconvhull = t[end] .* âˆ‚F_elt(pb, state.x) .+ sum(t[i] .* âˆ‚F_elt(pb, state.xs[i]) for i in 1:gs.m)
    @objective(model, Min, dot(gconvhull, gconvhull))
    @constraint(model, sum(t) == 1)

    optimize!(model)

    if termination_status(model) âˆ‰ Set([MOI.OPTIMAL, MOI.SLOW_PROGRESS])
        @warn "ComProx: subproblem of it $n was not solved to optimality" termination_status(model) primal_status(model) dual_status(model)
    end

    gáµ = value.(gconvhull)
    gáµ_norm = norm(gáµ)

    ## 3. termination
    if gáµ_norm â‰¤ gs.Î½_opt && state.Ïµâ‚– â‰¤ gs.Ïµ_opt
        iteration_status = problem_solved
    end

    ## 4. Update parameters
    Î½_next = state.Î½â‚–
    Ïµ_next = state.Ïµâ‚–
    tâ‚– = 1.0
    it_ls = 0
    if gáµ_norm â‰¤ state.Î½â‚–
        Î½_next = gs.Î¸_Î½ * state.Î½â‚–
        Ïµ_next = gs.Î¸_Ïµ * state.Ïµâ‚–
        tâ‚– = 0.0
    else
        Î½_next = state.Î½â‚–
        Ïµ_next = state.Ïµâ‚–
        tâ‚– = 1.0

        fâ‚– = F(pb, state.x)
        while !(F(pb, state.x - tâ‚– * gáµ) < fâ‚– - gs.Î² * tâ‚– * gáµ_norm^2) && (it_ls < gs.ls_maxit)
            tâ‚– *= gs.Î³
            it_ls += 1
        end

        if it_ls == gs.ls_maxit
            @warn("GradientSampling(): linesearch exceeded $(gs.ls_maxit) iterations, no suitable steplength found.")
        end
    end

    x_next = state.x - tâ‚– * gáµ
    if !is_differentiable(pb, x_next)
        @warn("Gradient sampling: F not differentiable at next point, portion to be imÃ¹plemented.")
        # throw(error("Gradient sampling: F not differentiable at next point, portion to be imÃ¹plemented."))
    end

    state.Ïµâ‚– = Ïµ_next
    state.Î½â‚– = Î½_next
    state.x = x_next
    state.k += 1

    return (Ïµâ‚–=state.Ïµâ‚–, Î½â‚–=state.Î½â‚–, it_ls=it_ls, gáµ_norm=gáµ_norm), iteration_status
end



get_minimizer_candidate(state::GradientSamplingState) = state.x