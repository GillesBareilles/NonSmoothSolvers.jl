
Base.@kwdef struct GradientSampling{Tf} <: Optimizer{Tf}
    m::Int64
    Î²::Tf = 1e-4
    Î³::Tf = 0.5
    Ïµ_opt::Tf = 1e-3
    Î½_opt::Tf = 1e-5
    Î¸_Ïµ::Tf = 0.1
    Î¸_Î½::Tf = 0.1
    ls_maxit::Int64 = 70
end

GradientSampling(initial_x::AbstractVector) = GradientSampling(m=length(initial_x)+1)

Base.@kwdef mutable struct GradientSamplingState{Tf} <: OptimizerState{Tf}
    x::Vector{Tf}
    âˆ‚gáµ¢s::Matrix{Tf}
    Ïµâ‚–::Tf
    Î½â‚–::Tf
    k::Int64 = 1
end

function initial_state(gs::GradientSampling{Tf}, initial_x::Vector{Tf}, pb) where {Tf}
    return GradientSamplingState(
        x = initial_x,
        âˆ‚gáµ¢s = zeros(Tf, length(initial_x), gs.m+1),
        Ïµâ‚– = Tf(0.1),
        Î½â‚– = Tf(0.1),
    )
end


#
### Printing
#
print_header(gs::GradientSampling) = println("**** GradientSampling algorithm\nm = $(gs.m)")

display_logs_header_post(gs::GradientSampling) = print("||gáµ||     Ïµâ‚–       Î½â‚–        it_ls")
function display_logs_post(os, gs::GradientSampling)
    @printf "%.3e  %.1e  %.1e  %2i" os.additionalinfo.gáµ_norm os.additionalinfo.Ïµâ‚– os.additionalinfo.Î½â‚– os.additionalinfo.it_ls
end


#
### GradientSampling method
#
"""
    update_iterate!(state, gs::GradientSampling, pb)

NOTE: each iteration is costly. This can be explored with NonSmoothProblems.to.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time                   Allocations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tot / % measured:                  45.5s / 2.89%           1.63GiB / 14.5%

Section                            ncalls     time   %tot     avg     alloc   %tot      avg
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GS 2. minimum norm (sub)gradient       20    1.30s  98.6%  64.9ms    240MiB  99.0%  12.0MiB
GS 4. Update parameters                20   12.8ms  0.97%   641Î¼s   1.63MiB  0.67%  83.6KiB
GS 5. diff check                       20   3.04ms  0.23%   152Î¼s    428KiB  0.17%  21.4KiB
GS 1. point sampling                   20   2.49ms  0.19%   124Î¼s    497KiB  0.20%  24.9KiB
GS 3. Termination                      20   33.8Î¼s  0.00%  1.69Î¼s      320B  0.00%    16.0B
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
function update_iterate!(state::GradientSamplingState{Tf}, gs::GradientSampling, pb) where Tf
    iteration_status = iteration_completed
    n = length(state.x)
    âˆ‚gáµ¢s = state.âˆ‚gáµ¢s

    @timeit_debug "GS 1. sampling points, eval gradients" begin
    ## 1. Sample m points in ð”¹(x, Ïµâ‚–)
    Random.seed!(123 + state.k)
    for i in 1:gs.m
        âˆ‚gáµ¢ = @view âˆ‚gáµ¢s[:, i]
        âˆ‚gáµ¢ .= rand(Normal(), n)
        âˆ‚gáµ¢ .*= state.Ïµâ‚– * rand()^(1/n) / norm(âˆ‚gáµ¢)
        âˆ‚gáµ¢ .+= state.x
        âˆ‚gáµ¢ .= âˆ‚F_elt(pb, âˆ‚gáµ¢)
    end
    âˆ‚gáµ¢s[:, gs.m+1] .= âˆ‚F_elt(pb, state.x)
    end

    ## 2. Find minimal norm element of convex hull at gradients of previous points.
    @timeit_debug "GS 2. minimum norm (sub)gradient" begin
    set = CHP.SimplexShadow(âˆ‚gáµ¢s)
    x0 = zeros(Tf, gs.m+1)

    Î±, str = CHP.optimize(set, x0)

    gáµ = âˆ‚gáµ¢s * Î±
    gáµ_norm = norm(gáµ)
    end


    ## 3. termination
    @timeit_debug "GS 3. Termination" begin
    if gáµ_norm â‰¤ gs.Î½_opt && state.Ïµâ‚– â‰¤ gs.Ïµ_opt
        iteration_status = problem_solved
    end
    end

    ## 4. Update parameters
    @timeit_debug "GS 4. Update parameters" begin
    Î½_next = state.Î½â‚–
    Ïµ_next = state.Ïµâ‚–
    tâ‚– = 1.0
    it_ls = 0
    if gáµ_norm â‰¤ state.Î½â‚–
        Î½_next = gs.Î¸_Î½ * state.Î½â‚–
        Ïµ_next = gs.Î¸_Ïµ * state.Ïµâ‚–
        tâ‚– = 0.0
    else
        Î½_next = state.Î½â‚–
        Ïµ_next = state.Ïµâ‚–
        tâ‚– = 1.0

        fâ‚– = F(pb, state.x)
        while !(F(pb, state.x - tâ‚– * gáµ) < fâ‚– - gs.Î² * tâ‚– * gáµ_norm^2) && (it_ls < gs.ls_maxit)
            tâ‚– *= gs.Î³
            it_ls += 1
        end

        if it_ls == gs.ls_maxit
            @warn("GradientSampling(): linesearch exceeded $(gs.ls_maxit) iterations, no suitable steplength found.")
        end
    end
    end

    @timeit_debug "GS 5. diff check" begin
    x_next = state.x - tâ‚– * gáµ
    if !is_differentiable(pb, x_next)
        @warn("Gradient sampling: F not differentiable at next point, portion to be implemented.")
    end

    state.Ïµâ‚– = Ïµ_next
    state.Î½â‚– = Î½_next
    state.x = x_next
    state.k += 1
    end

    return (;
            Ïµâ‚– = state.Ïµâ‚–,
            Î½â‚– = state.Î½â‚–,
            it_ls,
            gáµ_norm,
            F = 2 + it_ls,              # orcale calls
            âˆ‚F_elt = gs.m+1,
            is_differentiable = 1,
            ), iteration_status
end



get_minimizer_candidate(state::GradientSamplingState) = state.x
